import asyncio
import boto3
import time
from typing import List, Dict, Any, AsyncGenerator, Optional
from dataclasses import dataclass
from enum import Enum
from concurrent.futures import ThreadPoolExecutor
import threading
from queue import Queue
import json
from langchain_aws import ChatBedrock
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

class OrignalLLM:
    """ê³ ê¸‰ LLM ìŠ¤íŠ¸ë¦¬ë° ê´€ë¦¬ì"""
    
    def __init__(self, region_name: str = "us-west-2"):
        self.region_name = region_name
        self.model_id = None
        self.bedrock_client = self._setup_bedrock_client()  # Initialize bedrock_client first!
        self.llm = self._setup_llm()
    
    def _setup_bedrock_client(self):
        """Bedrock í´ë¼ì´ì–¸íŠ¸ ì„¤ì •"""
        try:
            return boto3.client(
                service_name='bedrock-runtime',
                region_name=self.region_name
            )
        except Exception as e:
            print(f"âŒ Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return None
    
    def _setup_llm(self, model_id="global.anthropic.claude-sonnet-4-20250514-v1:0"):
        """LLM ì„¤ì •"""
        try:
            
            if not self.bedrock_client:
                print("âŒ Bedrock í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None
            self.model_id = model_id
            return ChatBedrock(
                client=self.bedrock_client,
                model_id=self.model_id,
                model_kwargs={
                    "max_tokens": 2000,
                    "temperature": 0.15,
                    "top_p": 0.9,
                }
            )
        except Exception as e:
            print(f"âŒ LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return None



async def extract_text(chain):
    """í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜"""
    collected_text = []
    try:
        async for event in chain.astream_events({}):
            if event["event"] == "on_chat_model_stream":
                chunk = event["data"]["chunk"]
                if hasattr(chunk, 'content') and chunk.content:
                    # Handle different content formats
                    if isinstance(chunk.content, str):
                        collected_text.append(chunk.content)
                        print(chunk.content, end="", flush=True)
                    elif isinstance(chunk.content, list):
                        for content_item in chunk.content:
                            if isinstance(content_item, dict):
                                if text := content_item.get('text'):
                                    collected_text.append(text)
                                    print(text, end="", flush=True)
                            elif isinstance(content_item, str):
                                collected_text.append(content_item)
                                print(content_item, end="", flush=True)
    except Exception as e:
        print(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: {e}")
        # Fallback to regular invoke
        try:
            result = await chain.ainvoke({})
            return result
        except:
            # Final fallback to sync
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(None, chain.invoke, {})
            return result
    
    return "".join(collected_text)

async def main():
    print("ğŸš€ LLM ì´ˆê¸°í™” ì¤‘...")
    agent = OrignalLLM()
    
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", "You are a helpful assistant."),
        ("human", "íƒœì–‘ì˜ ì˜¨ë„ì— ëŒ€í•´ ë§í•´ì¤˜")
    ])
    
    chain = prompt | agent.llm | StrOutputParser()
    
    print("ğŸ”¥ ì‘ë‹µ ìƒì„± ì¤‘...")
    try:
        full_text = await extract_text(chain)
        print(f"\n\nâœ… ì™„ë£Œ! ì´ {len(full_text)} ê¸€ì ìƒì„±ë¨")
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        # Fallback
        try:
            result = chain.invoke({})
            print(f"âœ… Fallback ì„±ê³µ: {result}")
        except Exception as fallback_error:
            print(f"âŒ Fallbackë„ ì‹¤íŒ¨: {fallback_error}")

if __name__ == "__main__":
    asyncio.run(main())







