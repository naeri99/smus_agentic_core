import asyncio
import boto3
from langchain_aws import ChatBedrock
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from bedrock_agentcore.runtime import BedrockAgentCoreApp
import json
app = BedrockAgentCoreApp()

class AdvancedLLM:
    """ê³ ê¸‰ LLM ìŠ¤íŠ¸ë¦¬ë° ê´€ë¦¬ì"""
    
    def __init__(self, region_name: str = "us-west-2"):
        self.region_name = region_name
        self.model_id = None
        self.bedrock_client = self._setup_bedrock_client()
        self.llm = self._setup_llm()
    
    def _setup_bedrock_client(self):
        """Bedrock í´ë¼ì´ì–¸íŠ¸ ì„¤ì •"""
        try:
            return boto3.client(
                service_name='bedrock-runtime',
                region_name=self.region_name
            )
        except Exception as e:
            print(f"âŒ Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return None
    
    def _setup_llm(self, model_id="global.anthropic.claude-sonnet-4-20250514-v1:0"):
        """LLM ì„¤ì •"""
        try:
            if not self.bedrock_client:
                print("âŒ Bedrock í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None
            self.model_id = model_id
            return ChatBedrock(
                client=self.bedrock_client,
                model_id=self.model_id,
                model_kwargs={
                    "max_tokens": 2000,
                    "temperature": 0.15,
                    "top_p": 0.9,
                }
            )
        except Exception as e:
            print(f"âŒ LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return None

# ì „ì—­ ë³€ìˆ˜
agent = None

@app.entrypoint
async def extract_text(payload):
    """í…ìŠ¤íŠ¸ ì¶”ì¶œ AgentCore Runtime ì—”íŠ¸ë¦¬í¬ì¸íŠ¸"""
    global agent
    
    if agent is None:
        yield {"type": "status", "message": "ğŸš€ LLM ì´ˆê¸°í™” ì¤‘..."}
        agent = AdvancedLLM()
    
    # payloadì—ì„œ ì…ë ¥ ë°ì´í„° ì¶”ì¶œ
    user_input = payload.get("input_data", "íƒœì–‘ì˜ ì˜¨ë„ì— ëŒ€í•´ ë§í•´ì¤˜")
    
    # ì²´ì¸ êµ¬ì„±
    prompt = ChatPromptTemplate.from_messages([
        ("system", "You are a helpful assistant."),
        ("human", user_input)
    ])
    
    chain = prompt | agent.llm | StrOutputParser()
    
    yield {"type": "status", "message": "ğŸ”¥ ì‘ë‹µ ìƒì„± ì¤‘..."}
    
    collected_text = []
    try:
        async for event in chain.astream_events({}):
            if event["event"] == "on_chat_model_stream":
                chunk = event["data"]["chunk"]
                if hasattr(chunk, 'content') and chunk.content:
                    if isinstance(chunk.content, str):
                        collected_text.append(chunk.content)
                        yield {"type": "stream", "content": chunk.content}
                    elif isinstance(chunk.content, list):
                        for content_item in chunk.content:
                            if isinstance(content_item, dict):
                                if text := content_item.get('text'):
                                    collected_text.append(text)
                                    yield {"type": "stream", "content": text}
                            elif isinstance(content_item, str):
                                collected_text.append(content_item)
                                yield {"type": "stream", "content": content_item}
                                
    except Exception as e:
        yield {"type": "error", "message": f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: {e}"}
        

    
    # ìµœì¢… ê²°ê³¼
    final_text = "".join(collected_text)
    yield {"type": "final", "content": final_text}

if __name__ == "__main__":
    app.run()









