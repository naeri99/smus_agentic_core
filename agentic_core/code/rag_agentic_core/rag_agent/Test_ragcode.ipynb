{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "083964f8-0ef6-4903-a7af-93d40189a402",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T00:52:08.062830Z",
     "iopub.status.busy": "2025-11-08T00:52:08.062316Z",
     "iopub.status.idle": "2025-11-08T00:52:11.389080Z",
     "shell.execute_reply": "2025-11-08T00:52:11.388441Z",
     "shell.execute_reply.started": "2025-11-08T00:52:08.062799Z"
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import boto3\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from bedrock_agentcore.runtime import BedrockAgentCoreApp\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from boto3.session import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "681c5d89-16cd-4bc7-bc8e-fe67861f80b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T00:52:12.217155Z",
     "iopub.status.busy": "2025-11-08T00:52:12.216815Z",
     "iopub.status.idle": "2025-11-08T00:52:12.231983Z",
     "shell.execute_reply": "2025-11-08T00:52:12.231066Z",
     "shell.execute_reply.started": "2025-11-08T00:52:12.217123Z"
    }
   },
   "outputs": [],
   "source": [
    "class OpenSearchEmbeddingProcessor:\n",
    "    \"\"\"OpenSearch ì„ë² ë”© ì²˜ë¦¬ ë° ì €ì¥ í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, index_name= \"aws-document-chunks\" ):\n",
    "        # AWS region\n",
    "\n",
    "        boto_session = Session()\n",
    "        region_name = boto3.Session().region_name\n",
    "\n",
    "        self.region = region_name\n",
    "        self.service = 'es'\n",
    "        \n",
    "        # AWS credential\n",
    "        self.session = boto3.Session()\n",
    "        self.credentials = self.session.get_credentials()\n",
    "        \n",
    "        # OpenSearch setting\n",
    "        secrets_client = boto3.client('secretsmanager', region_name=self.region)\n",
    "        response = secrets_client.get_secret_value(SecretId='opensearch-credentials')\n",
    "        secrets = json.loads(response['SecretString'])\n",
    "        self.username = secrets['username']\n",
    "        self.password = secrets['password'] \n",
    "        self.host = secrets['opensearch_host']\n",
    "        self.headers = {'Content-Type': 'application/json'}\n",
    "        \n",
    "        # embedding\n",
    "        self.embeddings = self._setup_embeddings()\n",
    "        self.os_client = OpenSearch(\n",
    "                            hosts=[{'host': self.host, 'port': 443}],\n",
    "                            http_auth=(self.username, self.password),\n",
    "                            use_ssl=True,\n",
    "                            verify_certs=True,\n",
    "                            connection_class=RequestsHttpConnection\n",
    "                        )\n",
    "        self.index_name = index_name\n",
    "\n",
    "    def _setup_embeddings(self):\n",
    "        \"\"\"Bedrock ì„ë² ë”© ëª¨ë¸ ì„¤ì •\"\"\"\n",
    "        try:\n",
    "            return BedrockEmbeddings(\n",
    "                client=boto3.client(\n",
    "                    service_name='bedrock-runtime',\n",
    "                    region_name=self.region\n",
    "                ),\n",
    "                model_id=\"amazon.titan-embed-text-v2:0\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        \"\"\"BedrockEmbeddingsë¥¼ ì‚¬ìš©í•œ ì„ë² ë”© ìƒì„±\"\"\"\n",
    "        if not self.embeddings:\n",
    "            self.embeddings = self._setup_embeddings()\n",
    "        return self.embeddings.embed_query(text)\n",
    "\n",
    "    def vector_search(self, query, k=5):\n",
    "        try:\n",
    "            # KNN vector search query\n",
    "            query_vector = self.get_embedding(query)\n",
    "            vector_search = {\n",
    "                \"size\": k,\n",
    "                \"_source\": {\n",
    "                    \"excludes\": [\"content_embedding\"]\n",
    "                },\n",
    "                \"query\": {\n",
    "                    \"knn\": {\n",
    "                        \"content_embedding\": {\n",
    "                            \"vector\": query_vector,\n",
    "                            \"k\": k\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Execute the search\n",
    "            response = self.os_client.search(\n",
    "                index=self.index_name,\n",
    "                body=vector_search\n",
    "            )\n",
    "\n",
    "            documents = []\n",
    "            for res in response[\"hits\"][\"hits\"]:\n",
    "                source = res['_source']\n",
    "                page_content = {k: source[k] for k in source if k != \"vector\"}\n",
    "                metadata = {\"id\": res['_id']}\n",
    "                score = res['_score']\n",
    "                documents.append((Document(page_content=json.dumps(page_content, ensure_ascii=False), metadata=metadata), score))\n",
    "            return documents\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Search error: {str(e)}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69773e39-7a7d-41e9-a34b-0d3e932df70c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T00:52:12.980058Z",
     "iopub.status.busy": "2025-11-08T00:52:12.979786Z",
     "iopub.status.idle": "2025-11-08T00:52:12.986022Z",
     "shell.execute_reply": "2025-11-08T00:52:12.985248Z",
     "shell.execute_reply.started": "2025-11-08T00:52:12.980037Z"
    }
   },
   "outputs": [],
   "source": [
    "class RagLLM:\n",
    "    \"\"\"RagLLM ìŠ¤íŠ¸ë¦¬ë° ê´€ë¦¬ì\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        boto_session = Session()\n",
    "        region_name = boto3.Session().region_name\n",
    "        self.region_name = region_name\n",
    "        self.model_id = None\n",
    "        self.bedrock_client = self._setup_bedrock_client()\n",
    "        self.llm = self._setup_llm()\n",
    "    \n",
    "    def _setup_bedrock_client(self):\n",
    "        \"\"\"Bedrock í´ë¼ì´ì–¸íŠ¸ ì„¤ì •\"\"\"\n",
    "        try:\n",
    "            return boto3.client(\n",
    "                service_name='bedrock-runtime',\n",
    "                region_name=self.region_name\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _setup_llm(self, model_id=\"global.anthropic.claude-sonnet-4-20250514-v1:0\"):\n",
    "        \"\"\"LLM ì„¤ì •\"\"\"\n",
    "        try:\n",
    "            if not self.bedrock_client:\n",
    "                print(\"âŒ Bedrock í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "                return None\n",
    "            self.model_id = model_id\n",
    "            return ChatBedrock(\n",
    "                client=self.bedrock_client,\n",
    "                model_id=self.model_id,\n",
    "                model_kwargs={\n",
    "                    \"max_tokens\": 2000,\n",
    "                    \"temperature\": 0.15,\n",
    "                    \"top_p\": 0.9,\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ca8cf47-4314-4a9a-bbf4-ac8ed63a1d68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T00:52:13.995179Z",
     "iopub.status.busy": "2025-11-08T00:52:13.994860Z",
     "iopub.status.idle": "2025-11-08T00:52:13.998609Z",
     "shell.execute_reply": "2025-11-08T00:52:13.997734Z",
     "shell.execute_reply.started": "2025-11-08T00:52:13.995152Z"
    }
   },
   "outputs": [],
   "source": [
    "agent = None\n",
    "opensearh = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f28aebef-3b9a-4a1b-b70d-e2adcfaf31da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T00:52:15.647743Z",
     "iopub.status.busy": "2025-11-08T00:52:15.647318Z",
     "iopub.status.idle": "2025-11-08T00:52:15.651944Z",
     "shell.execute_reply": "2025-11-08T00:52:15.651115Z",
     "shell.execute_reply.started": "2025-11-08T00:52:15.647698Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_prompt():\n",
    "    template_lambda = \"\"\"The following is a friendly conversation between a human and an AI. \n",
    "    The AI is talkative and provides lots of specific details from its context. \n",
    "    If the AI does not know the answer to a question, it truthfully says it does not know. \n",
    "    The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
    "    \n",
    "    Relevant Information:\n",
    "    {document}\n",
    "    \n",
    "    Conversation:\n",
    "    Human: {question}\n",
    "    AI:\"\"\"\n",
    "    \n",
    "    prompt_lambda = PromptTemplate(\n",
    "        input_variables=[\"document\", \"question\"], \n",
    "        template=template_lambda\n",
    "    )\n",
    "    \n",
    "    return prompt_lambda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d94c569d-604e-485a-a784-b67a9343f7f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T00:52:16.427305Z",
     "iopub.status.busy": "2025-11-08T00:52:16.427043Z",
     "iopub.status.idle": "2025-11-08T00:52:16.436630Z",
     "shell.execute_reply": "2025-11-08T00:52:16.435948Z",
     "shell.execute_reply.started": "2025-11-08T00:52:16.427284Z"
    }
   },
   "outputs": [],
   "source": [
    "async def extract_text(payload):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ì¶”ì¶œ AgentCore Runtime ì—”íŠ¸ë¦¬í¬ì¸íŠ¸\"\"\"\n",
    "    global agent\n",
    "    global opensearh\n",
    "    \n",
    "    if agent is None:\n",
    "        yield {\"type\": \"status\", \"message\": \"ğŸš€ LLM ì´ˆê¸°í™” ì¤‘...\"}\n",
    "        agent = RagLLM()\n",
    "    if opensearh is None:\n",
    "        yield {\"type\": \"status\", \"message\": \"ğŸš€ Opensearch Connection ì´ˆê¸°í™” ì¤‘...\"}\n",
    "        opensearh = OpenSearchEmbeddingProcessor()\n",
    "    \n",
    "    # payloadì—ì„œ ì…ë ¥ ë°ì´í„° ì¶”ì¶œ\n",
    "    user_input = payload.get(\"input_data\", \"íƒœì–‘ì˜ ì˜¨ë„ì— ëŒ€í•´ ë§í•´ì¤˜\")\n",
    "\n",
    "    rag_prompt = get_prompt()\n",
    "\n",
    "    chain_lambda_rag = (\n",
    "        {\n",
    "            \"document\": lambda x: opensearh.vector_search(user_input),\n",
    "            \"question\": itemgetter(\"question\")\n",
    "        }\n",
    "        | rag_prompt\n",
    "        | agent.llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    yield {\"type\": \"status\", \"message\": \"ğŸ”¥ ì‘ë‹µ ìƒì„± ì¤‘...\"}\n",
    "    \n",
    "    collected_text = []\n",
    "    try:\n",
    "        async for event in chain_lambda_rag.astream_events({\"question\": user_input}):\n",
    "            if event[\"event\"] == \"on_chat_model_stream\":\n",
    "                chunk = event[\"data\"][\"chunk\"]\n",
    "                if hasattr(chunk, 'content') and chunk.content:\n",
    "                    if isinstance(chunk.content, str):\n",
    "                        collected_text.append(chunk.content)\n",
    "                        yield {\"type\": \"stream\", \"content\": chunk.content}\n",
    "                    elif isinstance(chunk.content, list):\n",
    "                        for content_item in chunk.content:\n",
    "                            if isinstance(content_item, dict):\n",
    "                                if text := content_item.get('text'):\n",
    "                                    collected_text.append(text)\n",
    "                                    yield {\"type\": \"stream\", \"content\": text}\n",
    "                            elif isinstance(content_item, str):\n",
    "                                collected_text.append(content_item)\n",
    "                                yield {\"type\": \"stream\", \"content\": content_item}\n",
    "                                \n",
    "    except Exception as e:\n",
    "        yield {\"type\": \"error\", \"message\": f\"âŒ ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: {e}\"}\n",
    "    \n",
    "    # ìµœì¢… ê²°ê³¼\n",
    "    final_text = \"\".join(collected_text)\n",
    "    yield {\"type\": \"final\", \"content\": final_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02e25cb-59b4-47c9-874d-6242c6d198a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb84fa29-7509-468d-a3c9-c492a5f6c528",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T00:52:56.937748Z",
     "iopub.status.busy": "2025-11-08T00:52:56.937273Z",
     "iopub.status.idle": "2025-11-08T00:53:04.664666Z",
     "shell.execute_reply": "2025-11-08T00:53:04.664009Z",
     "shell.execute_reply.started": "2025-11-08T00:52:56.937711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ ì‘ë‹µ ìƒì„± ì¤‘...\n",
      "AWS S3 Glacierì€ Amazon S3ì˜ ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ ì¤‘ í•˜ë‚˜ë¡œ, ì•„ì¹´ì´ë¸Œ ë° ì¥ê¸° ë°ì´í„° ë³´ê´€ì„ ìœ„í•œ ì €ë¹„ìš© ìŠ¤í† ë¦¬ì§€ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.\n",
      "\n",
      "ì œê³µëœ ì •ë³´ì— ë”°ë¥´ë©´, S3 Glacierì—ëŠ” ë‘ ê°€ì§€ ì£¼ìš” ì˜µì…˜ì´ ìˆìŠµë‹ˆë‹¤:\n",
      "\n",
      "1. **S3 Glacier Instant Retrieval**: ì¦‰ê°ì ì¸ ì•¡ì„¸ìŠ¤ê°€ í•„ìš”í•œ ì•„ì¹´ì´ë¸Œ ë°ì´í„°ë¥¼ ìœ„í•œ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.\n",
      "\n",
      "2. **S3 Glacier Flexible Retrieval** (ì´ì „ Amazon Glacier): ì¦‰ê°ì ì¸ ì•¡ì„¸ìŠ¤ê°€ í•„ìš”í•˜ì§€ ì•Šì€ ë“œë¬¼ê²Œ ì•¡ì„¸ìŠ¤í•˜ëŠ” ì¥ê¸° ë°ì´í„°ë¥¼ ìœ„í•œ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.\n",
      "\n",
      "ë˜í•œ ë” ì €ë ´í•œ ì˜µì…˜ìœ¼ë¡œ **Amazon Glacier Deep Archive**ë„ ìˆëŠ”ë°, ì´ëŠ” í´ë¼ìš°ë“œì—ì„œ ê°€ì¥ ì €ë ´í•œ ìŠ¤í† ë¦¬ì§€ë¡œ ëª‡ ì‹œê°„ ë§Œì— ê²€ìƒ‰í•  ìˆ˜ ìˆëŠ” ì¥ê¸° ì•„ì¹´ì´ë¸Œ ë° ë””ì§€í„¸ ë³´ì¡´ì„ ìœ„í•œ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.\n",
      "\n",
      "ì´ëŸ¬í•œ Glacier ì„œë¹„ìŠ¤ë“¤ì€ Amazon S3ì˜ 99.999999999%(11 9s) ë‚´êµ¬ì„±ì„ ì œê³µí•˜ë©°, ë°±ì—… ë° ë³µì›, ì•„ì¹´ì´ë¸Œ ë“±ì˜ ìš©ë„ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. íŠ¹íˆ ìì£¼ ì•¡ì„¸ìŠ¤í•˜ì§€ ì•Šì§€ë§Œ ì¥ê¸°ê°„ ë³´ê´€í•´ì•¼ í•˜ëŠ” ë°ì´í„°ì— ì í•©í•˜ë©°, ë‹¤ì–‘í•œ ì•¡ì„¸ìŠ¤ íŒ¨í„´ê³¼ ë¹„ìš© ìš”êµ¬ì‚¬í•­ì— ë§ì¶° ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ìµœì¢… ê²°ê³¼:\n",
      "AWS S3 Glacierì€ Amazon S3ì˜ ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ ì¤‘ í•˜ë‚˜ë¡œ, ì•„ì¹´ì´ë¸Œ ë° ì¥ê¸° ë°ì´í„° ë³´ê´€ì„ ìœ„í•œ ì €ë¹„ìš© ìŠ¤í† ë¦¬ì§€ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.\n",
      "\n",
      "ì œê³µëœ ì •ë³´ì— ë”°ë¥´ë©´, S3 Glacierì—ëŠ” ë‘ ê°€ì§€ ì£¼ìš” ì˜µì…˜ì´ ìˆìŠµë‹ˆë‹¤:\n",
      "\n",
      "1. **S3 Glacier Instant Retrieval**: ì¦‰ê°ì ì¸ ì•¡ì„¸ìŠ¤ê°€ í•„ìš”í•œ ì•„ì¹´ì´ë¸Œ ë°ì´í„°ë¥¼ ìœ„í•œ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.\n",
      "\n",
      "2. **S3 Glacier Flexible Retrieval** (ì´ì „ Amazon Glacier): ì¦‰ê°ì ì¸ ì•¡ì„¸ìŠ¤ê°€ í•„ìš”í•˜ì§€ ì•Šì€ ë“œë¬¼ê²Œ ì•¡ì„¸ìŠ¤í•˜ëŠ” ì¥ê¸° ë°ì´í„°ë¥¼ ìœ„í•œ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.\n",
      "\n",
      "ë˜í•œ ë” ì €ë ´í•œ ì˜µì…˜ìœ¼ë¡œ **Amazon Glacier Deep Archive**ë„ ìˆëŠ”ë°, ì´ëŠ” í´ë¼ìš°ë“œì—ì„œ ê°€ì¥ ì €ë ´í•œ ìŠ¤í† ë¦¬ì§€ë¡œ ëª‡ ì‹œê°„ ë§Œì— ê²€ìƒ‰í•  ìˆ˜ ìˆëŠ” ì¥ê¸° ì•„ì¹´ì´ë¸Œ ë° ë””ì§€í„¸ ë³´ì¡´ì„ ìœ„í•œ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.\n",
      "\n",
      "ì´ëŸ¬í•œ Glacier ì„œë¹„ìŠ¤ë“¤ì€ Amazon S3ì˜ 99.999999999%(11 9s) ë‚´êµ¬ì„±ì„ ì œê³µí•˜ë©°, ë°±ì—… ë° ë³µì›, ì•„ì¹´ì´ë¸Œ ë“±ì˜ ìš©ë„ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. íŠ¹íˆ ìì£¼ ì•¡ì„¸ìŠ¤í•˜ì§€ ì•Šì§€ë§Œ ì¥ê¸°ê°„ ë³´ê´€í•´ì•¼ í•˜ëŠ” ë°ì´í„°ì— ì í•©í•˜ë©°, ë‹¤ì–‘í•œ ì•¡ì„¸ìŠ¤ íŒ¨í„´ê³¼ ë¹„ìš© ìš”êµ¬ì‚¬í•­ì— ë§ì¶° ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "async for result in extract_text({\"input_data\": \"AWS s3 glacierì€ ì–´ë–¤ ì„œë¹„ìŠ¤ì¸ê°€ìš”\"}):\n",
    "    if result['type'] == 'status':\n",
    "        print(result['message'])\n",
    "    elif result['type'] == 'stream':\n",
    "        print(result['content'], end='')\n",
    "    elif result['type'] == 'final':\n",
    "        print(f\"\\n\\nìµœì¢… ê²°ê³¼:\\n{result['content']}\")\n",
    "    elif result['type'] == 'error':\n",
    "        print(f\"ì˜¤ë¥˜: {result['message']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad8807e-3bf5-4d43-99ec-89145aeef04b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
